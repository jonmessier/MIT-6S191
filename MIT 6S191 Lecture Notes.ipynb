{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMNFTvSt1690FJpHDt+0Ifc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lecture 1\n","- Intro to Deep Learning\n","\n","## The perceptron - single neuron model\n"," - key component in deep learning modeling.  Comes up over and over as the base model.  \n","\n","###Foward Propagation\n","- $x_i$ - inputs\n","- $w_i$ - weights\n","- $w_0$ - bias\n","- $g$ - non-linear activation function like 'relu', 'sigmoid'\n","- $m$ - number of inputs\n","- $\\hat{y}=g(w_o + \\sum_{i=1}^m{x_iw_i}$)\n","- See \"How sum vectors and matrices work\" for proof\n","\n","###Common activation functions:\n","- Purpose of activation functions is to introduce non-linearities into the system.\n"," - Linear activation produces linear decisions no matter the network size\n","\n","- Sigma:\n"," - $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n"," - $\\frac{d\\sigma}{dx} = \\sigma(x) \\cdot (1 - \\sigma(x))$\n","- Relu:\n"," - $\\text{ReLU}(x) = \\max(0, x)$\n"," - $\\frac{d\\text{ReLU}}{dx} = \\begin{cases} 0 & \\text{if } x < 0 \\\\ 1 & \\text{if } x \\geq 0 \\end{cases}$\n","- Hyperbolic tangent (tanh)\n"," - $\\text{tanh}(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n"," - $\\frac{d\\text{tanh}}{dx} = 1 - \\text{tanh}^2(x)$\n","\n","### Peceptron simplified\n","- single perceptron\n"," - $z=w_0 + \\sum_{j=1}^m{x_jw_j}$\n","\n","- Multi-ouput perceptron\n"," - $z_i=w_{0,i} + \\sum_{j=1}^m{x_jw_{j,i}}$\n"," - because all inputs are connected to all outputs, a MOP layer is called \"Dense\"\n"," - **Tensorflow**\n"," ```\n"," import tensorflow as tf\n"," layer = tf.keras.layers.Dense(units = \"m-1\")\n"," ```\n","\n","## Dense Layer\n","Defined as all inputs are connected to all outputs.  This is the multi-ouput perceptron model\n","\n","- in the slideshow they use an 'X' inside of a box to simplify the connections between inputs and nodes.\n","\n","## Multiple layers - \"Deep Neural Network\"\n","**Sequential Model** - you can use the outputs from one peceptron layer as the input for another layer\n","- $z_{k,i} = w_{0,i}^k+\\sum_{j=1}^{n_{k-1}}g(z_{k-1,j})w_{j,i}^k $\n","- See TF documentation [HERE](href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable')\n"," ```\n"," import tensorflow as tf\n"," model =tf.keras.Squential([tf.keras.laters.Dense(n1),\n","                            tf.keras.laters.Dense(n2),\n","                            ...\n","                            tf.keras.laters.Dense(2))]\n"," ```\n","\n","## Loss\n","Did my neural network make a mistake?  If so, how big was the mistake?  This is the amount of Loss\n","- \"Loss Function\" = \"Objective function\" = \"Empirical risk\" = \"Cost function\"\n","- $J(W) = \\frac{1}{n}\\sum_{i=1}^n L(f(x^i;W),y^i)$\n","- Minimize the mistakes of whole data set\n","\n","### Loss Functions\n","\n","Here are descriptions, formulas, LaTeX commands, and Python API calls for five common loss functions:\n","\n","1. **Mean Squared Error (MSE) Loss:**\n","   - Used for regression models that output coninuous real numbers\n","   - **Formula:**\n","   - $$  L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n","   - **TensorFlow API Call:**\n","     ```python\n","     mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n","     print(f'Mean Squared Error Loss: {mse_loss.numpy()}')\n","     ```\n","\n","2. **Binary Cross-Entropy Loss:**\n","   - This is the binary cross-entropy loss, and it's commonly used as the loss function for binary classification problems in machine learning\n","   - can be used with models that output probability between 0 and 1\n","   - **Formula:**\n","    $$L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right]$$\n","    \n","   - **TensorFlow API Call:**\n","     ```python\n","     # Calculate binary cross-entropy loss\n","     bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n","\n","     print(f'Binary Cross-Entropy Loss: {bce_loss.numpy()}')\n","     ```\n","\n","3. **Categorical Cross-Entropy Loss:**\n","   - The categorical cross-entropy loss function is commonly used in machine learning tasks where the goal is to classify instances into multiple classes.\n","   - **Formula:**\n","     $$L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\cdot \\log(\\hat{y}_{ij})\n","     $$\n","   - **TensorFlow API Call:**\n","     ```python\n","     # Calculate categorical cross-entropy loss\n","     cce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n","\n","     print(f'Categorical Cross-Entropy Loss: {cce_loss.numpy()}')\n","     ```\n","\n","4. **Hinge Loss (SVM):**\n","   -\n","   - **Formula:**\n","     $$\n","     L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y}_i)\n","     $$\n","   - **TensorFlow API Call:**\n","     ```python\n","     # Calculate hinge loss\n","     hinge_loss = tf.keras.losses.hinge(y_true, y_pred)\n","\n","     print(f'Hinge Loss: {hinge_loss.numpy()}')\n","     ```\n","\n","5. **Huber Loss:**\n","   - **Formula:**\n","\n","     $$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\begin{cases} \\frac{1}{2} (\\hat{y}_i - y_i)^2 & \\text{if } | \\hat{y}_i - y_i | \\leq \\delta \\\\ \\delta \\cdot | \\hat{y}_i - y_i | - \\frac{1}{2} \\delta^2 & \\text{otherwise} \\end{cases}\n","\n","     $$\n","   - **TensorFlow API Call:**\n","     ```python\n","     # Set the delta parameter (threshold for the absolute difference)\n","     delta = 1.0\n","\n","     # Calculate Huber loss\n","     huber_loss = tf.keras.losses.huber(y_true, y_pred, delta)\n","\n","     print(f'Huber Loss: {huber_loss.numpy()}')\n","     ```\n","\n","## Gradient Descent\n","### Algorithm\n","Sure, here's a step-by-step description of the gradient descent algorithm in a list format:\n","\n","1. **Initialize Parameters:**\n","   - Start with initial values for the parameters (weights and biases) of the model. This could be set randomly or using some predefined strategy.\n","\n","2. **Set Hyperparameters:**\n","   - Choose hyperparameters such as the learning rate (\\(\\alpha\\)), which determines the size of the steps taken during each iteration.\n","\n","3. **Compute Predictions:**\n","   - Use the current parameters to make predictions on the training data.\n","\n","4. **Compute Loss:**\n","   - Calculate the loss, which is a measure of how well the model is performing. The loss function depends on the specific task (e.g., mean squared error for regression, cross-entropy for classification).\n","\n","5. **Compute Gradients:**\n","   - Calculate the gradients of the loss with respect to each parameter. This involves computing the partial derivatives of the loss function with respect to each parameter.\n","\n","6. **Update Parameters:**\n","   - Adjust the parameters in the opposite direction of the gradients to minimize the loss. This is done using the update rule:\n","     \\[ \\text{parameter} = \\text{parameter} - \\alpha \\times \\text{gradient} \\]\n","   - Repeat this process for all parameters.\n","\n","7. **Iterate:**\n","   - Repeat steps 3-6 for a specified number of iterations (epochs) or until convergence. Convergence can be determined by observing changes in the loss or the gradients falling below a certain threshold.\n","\n","8. **Evaluate Convergence:**\n","   - Check for convergence by monitoring the changes in the loss or gradients. If the algorithm has converged, the process can be stopped.\n","\n","9. **Make Predictions:**\n","   - Use the trained model with the optimized parameters to make predictions on new, unseen data.\n","\n","This list captures the essence of the basic gradient descent algorithm. There are variations, such as stochastic gradient descent (SGD) and mini-batch gradient descent, which involve updating parameters based on a subset of the training data rather than the entire dataset at each iteration. These variations are often used to speed up the convergence process, especially for large datasets.\n","\n","### Computing gradients: Backpropagation\n","- In theory very simple: It's just the extantiation of the chain rule over and over\n","General Formaula\n","$$ \\frac{\\partial{J(W)}}{\\partial{w_2}} = \\frac{\\partial{J(W)}}{\\partial{\\hat{y}}} * \\frac{\\partial{\\hat{y}}}{\\partial{w_2}} $$\n","\n","Example:\n","$$\\frac{\\partial{J(W)}}{\\partial{w_1}} = \\frac{\\partial{J(W)}}{\\partial{\\hat{y}}} * \\frac{\\partial{\\hat{y}}}{\\partial{z_1}}*\\frac{\\partial{z_1}}{\\partial{w_1}} $$\n","\n","### Learning Rate\n","- How do you pick a good value?\n"," - If its too low: small learning rate and can get you stuck in a local minimum\n"," - If it's to high: we can get divergence\n"," - Often it's an exhaustive function of comparing different learning rate outputs.  This is BAD\n","\n","- Adaptive Learning Rate Algorithms\n"," - SGD - `tf.keras.optomizers.SDG `\n"," - Adam - `tf.keras.optomizers.Adam `\n"," - Adadelta - `tf.keras.optomizers.Adadelta `\n"," - Adagrad - `tf.keras.optomizers.Adagrad `\n"," - RMSProp - `tf.keras.optomizers.RMSProp`\n","\n","### Problem with gradient descent\n","- Too much computation\n","- Alternative - Use a single example in the data set- Stochastic Gradient descent.  this may be very noise.  Very fast\n","- Mini-batch gradient descent\n"," - Computationally efficient\n"," - reduces stochasticicity\n"," - batch sizes on the order of 10s or 100s of data points\n"," - smoother convergence\n"," - allows for parallization\n","\n","\n","### Overfitting\n","- When the model is too complex and does not generalize well\n","- **Regularization** - Technique that contrains our optomization to discourage complex models\n","- Neural networks are prone to overfitting\n","- **Dropout** - during training randomly set some activations to 0.  \n","  - forces the netowrk to not rely on any 1 node\n","  - **Tensorflow** - `tf.keras.layers.Dropout(p=0.5) #50% dropout`\n","- **Early Stopping**  stop training before overfitting occurs\n"," -\n","\n","\n"],"metadata":{"id":"BCuCuvT6QfUX"}},{"cell_type":"markdown","source":["## Todo List\n","- [ ] Read more on Common loss functions\n","- [ ] Compare and contrast Adaptive Learning Algorithms\n","\n"],"metadata":{"id":"zVlHDUdHx2ZA"}},{"cell_type":"markdown","source":["##Dense Layer from stratch"],"metadata":{"id":"FdxEqXnLvkGn"}},{"cell_type":"code","source":["### Dense Layer from Scratch ###\n","\n","# n_output_nodes: number of output nodes\n","# input_shape: shape of the input\n","# x: input to the layer\n","\n","class OurDenseLayer(tf.keras.layers.Layer):\n","  def __init__(self, n_output_nodes):\n","    super(OurDenseLayer, self).__init__()\n","    self.n_output_nodes = n_output_nodes\n","\n","  def build(self, input_shape):\n","    d = int(input_shape[-1])\n","    # Define and initialize parameters: a weight matrix W and bias b\n","    # Note that parameter initialization is random!\n","    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes]) # note the dimensionality\n","    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes]) # note the dimensionality\n","\n","  def call(self, x):\n","    '''TODO: define the operation for z (hint: use tf.matmul)'''\n","    z = tf.matmul(x,self.W) + self.b #TODO -done\n","    '''TODO: define the operation for out (hint: use tf.sigmoid)'''\n","    y = tf.sigmoid(z)# TODO -done\n","    return y"],"metadata":{"id":"1HlbjQKtbNag"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Gradient Descent Loop"],"metadata":{"id":"wu5zqGGMvrDp"}},{"cell_type":"code","source":["import tensorflow as tf\n","weights = tf.variable([tf.random.normal()])\n","while True:\n","  with tf.GradientTape() as g:\n","    loss = compute_loss(weights)\n","    gradient = g.gradient(loss,weights)\n","\n","  weights= weights- lr*gradient"],"metadata":{"id":"7zNhGyTMvuB2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Lecture 2 - Deep Sequence Modeling\n","Example Sequence Modeling Application\n","- Many-to-one - Sentiment Classification\n","- One to Many - Image Captioning\n","- Many to Many - Mchine Translation\n","\n","## Reccurent Neural Networks(RNNs)\n","Recurrance relation - something about what the network is computing a particular time is being passed to the other time/state-dependant instances\n","- $h_i$ - Internal state. Nueron memory\n","- This means that the network output is now dependant on the input $x$ and the state $h_i$\n"," - $\\hat{y}_t = f(x_t,h_{t-1})$\n"," - $h_t$ iteritively updated over time.\n","- recurrance relation - determins how $h_t$ updates.\n"," - $h_t=f_w(x_t,h_{t-1})$, where\n"," - $h_t$ - cell state\n"," - $f_w$ - function with weights W\n"," - $x_t$ - input\n"," - $h_{t-1}$ - old state\n","- Becuase we are now recording the state $h_i$ we are also creating a weight matrix corresponding to the state.\n"," 1. Output Vector - $\\hat{y}_t=W_{hy}^Th_t$\n"," 2. Update hidden state - $h_t = tanh(W_{hh}^Th_t-1+W_{xh}^Tx_t)$\n"," 3. Input vector $x_t$\n","\n","Another way to think about RNNs is a \"computational graph across time\".  Basically for each input there is an input $x_i$; a weight matrix $W_xh$ for the input to the RNN state, a weight matrix $W_{hy}$ for the RNN state to the output; and a weight matrix $W_{hh}$ for the RNN state to the next state.  These weight matrices are iteration independant. See RNN from stratch\n","\n","**Implementation in TensorFlow** - ``` tf.keras.layers.SimpleRNN(rnn_units) ```\n","\n","## Sequence Modeling design criteria\n","Need to:\n","1. Handle variable length sequences\n","2. Track long dependencies\n","3. Maintain information about order\n","4. Share parameters across the sequence\n","RNNS meet these criteria\n","\n","## RNNs Backpropagation through time\n","Computationally very costly because it requires calculating the gradient for all inputs, and weight matricies for each state.  \n","\n","- Many values of $l$ - leads to \"exploding gradients\"\n"," - fix with gradient clipping\n","- Few values of $l$ - leads to vanishing gradients\n"," - fix with activation function, weight initialization, or network architecture\n","\n","\n","### Dealing with Vanishing gradient problem\n","1. ReLU activation function - The RELU activation function has a derivative of 1, and thus the the gradient does not reduce below 1\n","2. Parameter initialization - Initial weights tot he identity matrix\n"," - Initialize biases to zero.  This helps weights from shrinking to zero\n","3. Gated Cells - use gates to selectively add or remove information withing each recurrent unit with a pointwise multiplcation and a activation function\n","\n","#### Long Short Term Memory (LSTMs)\n","Key concepts\n","1. Maintain a cell state\n","2. Use gates to control the flow of information\n"," - **Forget** gate gets rid of irrelevant info\n"," - **Store** relevant info from current input\n"," - **Update** cell state\n"," - **Output** gate returns a filtered version of the cell state\n","3. Backpropagation through time with partiall uniterrupted gradient flow\n","\n","- Flow\n"," 1 Forget --> 2. Store --> 3. Update --> 4. Output\n","\n","- LSTMs cells can track information through many timesteps\n","- **LSTMs in TensorFlow** - `tf.keras.layers.LSTM(num_units)`\n","\n"],"metadata":{"id":"IOcdmxfib214"}},{"cell_type":"markdown","source":["## RNN from scratch"],"metadata":{"id":"s04xFD3hHPjP"}},{"cell_type":"code","source":["class MyRNNCell(tf.keras.layers.Layer):\n","  def __init__(self,rnn_units, input_dim, output_dim):\n","    super(MyRNNCell,self).__init__()\n","\n","    #initialize weight matrices\n","    self.W_xh = self.add_weight([rnn_units,input_dim])\n","    self.W_hh = self.add_weight([rnn_units,rnn_units])\n","    self.W_hy = self.add_weight([output_dim,rnn_units])\n","\n","    #initiate the hidden state to zeros\n","    self.h = tf.zeros([rnn_units,1])\n","\n","  def call(self,x):\n","    #update the hidden state\n","    self.g=tf.math.tanh(self.W_hh*self.h*delf.W-xh*x)\n","\n","    #compute the output\n","    output = self.W_hy* self.h\n","\n","    #return the current output and hidden state\n","    return output, self.h"],"metadata":{"id":"kXQjqLT8HOdS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Lecture 3"],"metadata":{"id":"z5AjVSUXBFje"}},{"cell_type":"markdown","source":["#Lecture 4: Deep Generative Modeling\n","## Supervised vs. Unsupervised ML\n","Supervised\n","- Data : (x,y)\n","- Goal - Learn function to map $xâ†’y$\n","- Example: classification, regression, object detection, emantic segmentation\n","\n","Unsupervised\n","- Data: x\n","- Learn hidden/underlying structure of data\n","- Examples: Clustering, feature or dimensionality reduction\n","\n","##Generative Modeling\n","Take as input training sample from some distribution and learn a model that represents that distribution\n","\n","**Debiasing**  - Process of mitigating or reducing biases present in the training data or model.  Some common debiassing approaches are\n"," - Diverse and Representative training data -\n"," - Bias detection and measurement\n"," - Data Augmentation and balancing\n"," - Adversarial Training\n"," - Regularization Techniques\n"," - Explainability and Interpretability\n"," - Post Processing Techniques\n"," - Continuous monitoring\n","\n","## Latent Variable models\n","### Autoencoders and Variational Autoenvoders (VAEs)\n","**Latent Variable** - a variable that is not directly observed but is inferred through the observation of other variables.\n","\n","**Autoencoders**\n","- Encoder - takes input data and transformes it into a compressed representation typically of lower dimensionality than the orignal input\n","- Decoder - takes the encoded representation and reconstructs the input data from it.  the goal is to generate an output that is as close to the input as possible.\n","- Loss Function - Auto encoder is optimized to minimize the difference between the input and reconstructed output.  A loss function measures the dissimilarity bwteen the input and the output.  Common loss functions include MSE, BCE\n","\n","**Variational Autoencoders**\n","A type of autoencoder that uses a probablistic approach to represent input data.  The key innovation of VAEs lies in their ability to generate new data points by sampling from a learned probability distribution in the latent space.\n"," - Encoder\n","  - Similar to a regular autoencoder, a VAE consists of an encoder neural network that maps the input data to a probabilistic distribution in the latent space.\n","  - Instead of directly outputting a fixed encoding, the encoder outputs parameters of a probability distribution (usually Gaussian or normal distribution) in the latent space. These parameters are the mean ($\\mu$) and the standard deviation ($\\sigma$).\n","  - computes $q_{\\phi}(Z|X) $\n"," - Sampling\n","  - During the training phase, the VAE introduces a stochastic element by sampling from the latent distribution represented by Î¼\\muÎ¼ and Ïƒ\\sigmaÏƒ. This step is crucial for generating diverse data samples\n"," - Decoder:\n","  - The sampled latent variable is then passed through the decoder, which reconstructs the input data. Like a regular autoencoder, the decoder attempts to generate an output that closely matches the original input.\n","  - computes $p_{\\theta}=(X|Z)$\n"," - Loss Function:\n","  - The loss function for a VAE consists of two components: a reconstruction loss (measuring how well the reconstructed data matches the input) and a regularization term that encourages the learned latent distribution to be close to a standard normal distribution. This regularization term is typically the Kullback-Leibler (KL) divergence.\n","   - The KL divergence term penalizes the model if the learned latent distribution deviates significantly from a standard normal distribution (Gaussian). This regularization encourages the VAE to learn a well-behaved and smooth latent space, making it more suitable for generating diverse and meaningful samples during the generation phase.\n","   - $$D(q_{\\phi}(Z|X) || p(z) = -\\frac{1}{2}\\sum_{j=0}^{k-1}(\\sigma_j+\\mu_j^2 - log \\sigma_j)$$\n","   - where $-log\\sigma_j$ is the KL-divergence between the two distributions\n","\n","#### VAE optomization\n","$L(\\phi,\\theta)$ = Reconstruction Loss + regularization term\n","\n","**Reconstruction Loss** e.g. $||x-\\hat{x}||^2$.  When normalizing use\n","\n","**Regularization term**: $D(q_{\\phi} (z|x) || p(z))$ where\n"," - $q_{\\phi} (z|x)$ - is the inferred latent distribution\n"," - $p(z)$ - Fixed propr on latent distribution\n","  - Common choice of prior - Normal Gaussian $$p(z) = N(\\mu=0,\\sigma^2=1)$$\n","  - encourages encodings to distrbute normall\n","  - penalizes the network when it \"tries to cheat\" by clustering points to specific regions\n"," -  What are we trying to achieve with regularization?\n","  1. Continuity - points that are close in the latent space produce similar content after decoding\n","  2. completeness - sampling from the laten space leads to meaningful content after decoding\n","  3. Regularization with Normal pior helps enfoce information gradient in the latent space\n","\n","#### VAE computation\n","- Problem: Cannot backpropagate gradients through sampling layers because of the probablistic approach.\n","- **Re-paramaterization** - redefine how a latent variable is sampled with a fixed \\mu and \\sigma vectors scaled by random constants drawn from the prior distribution\n"," $$\\Rightarrow z = \\mu +\\sigma\\odot\\epsilon$$\n","- $\\epsilon$ is drawn from normal distribution\n","- Review slide 35 for diagram on process\n","\n","#### VAEs Latent perturbation\n","- slowly increas or decrease a single latent variable and keep all other variables constant\n","- Ideally we want latent variables that are uncorrelated.\n"," - enforce diagonal prior on the latent variables to encourage independence.\n","- $\\beta VAE$ - introduce a loss function $\\beta$ that controls the level of entanglement.  $\\beta=1$ means no separation.  $\\beta>1 \\rightarrow$ constrains latent bottleneck\n","\n","### Generative Adversarial Network (GANs)\n","- What if we only focus on the quality of the generated sample?  \n"," - Goal to generate new instances that are close to input\n"," - It can be difficult to learn the distribution directly.  Instead sample from simple random noise and \"learn\" the transformation\n","\n","**GANs**\n","- have 2 neural networks.  \n"," - The **Generator** turns noise into an imitation of the data to try and trick the discriminator.  \n"," - The **discriminator** tries to determine the real vs. fake data\n","\n","**Chat GPT** notes for step by step description of training process:\n","#### Training Process:\n","- During each iteration, the generator generates synthetic data, and the discriminator evaluates both real and generated data.\n","- The gradients from the discriminator's loss are used to update the discriminator's parameters, making it better at distinguishing real from fake data.\n","- The gradients from the generator's loss are used to update the generator's parameters, making it better at generating data that fools the discriminator.\n","- This process continues in a feedback loop, with the generator and discriminator getting better at their respective tasks over time.\n","\n","####Convergence:\n","- Ideally, the GAN reaches a point where the generator produces data that is indistinguishable from real data, and the discriminator cannot reliably tell the difference.\n","- At this point, the GAN is said to have reached convergence.\n","\n","#### GAN loss function\n","- **D** tries to identify the synthesized images via $$ arg max(D)E_{z,x}[logD(G(z))+log(1-D(X))] $$\n","- **G** tries to synthesize fake images that fool **D** $$ arg min(G)E_{z,x}[logD(G(z))+log(1-D(X))] $$\n","- Combine the two processes - $$argmin(G)max(D)E_{z,x}[logD(G(z))+log(1-D(X))]$$\n","\n","#### Conditional GANS\n","- add a factor $c$ applied to the noise and the Descriminator $D$\n","\n","### Cycle Gan\n","Chat GPT description:\n","CycleGAN, short for Cycle-Consistent Generative Adversarial Network, is a type of generative model designed for image-to-image translation without paired training data. It was introduced by Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros in 2017. CycleGAN is particularly powerful for tasks where there's a lack of paired data, such as transforming images from one domain to another without explicit correspondences.\n","\n","- In audio:  Take the spectrum of words from speech, apply a synthesis to a dirfferent domain.  i.e. instructors voice to obama"],"metadata":{"id":"3y-TH5mBBJ9k"}},{"cell_type":"markdown","source":["#Lecture 5: Robust and Trustworthy Deep Learning\n","**BIAS** - What happens when models are skewed by sensative feature inputs? i.e. skin color\n","\n","**UNCERTAINTY** - Can we teach a model to rexognize when it doesnt know an answer?\n","\n","## Bias present in the AI lifecycle\n","1. Sampling bias - over sampling from one area of data, undersampling from another.  i.e. healthy patients vs. diseased.  \n","2. Lack of uncertainty benchmarks and metrics\n","3. Deployment - what happens when a model deployed in 2023 is used in 2035?  Probably wont work\n","4. Evaluation - Bulk metrics dont apply to subgroups.  \n","\n","### Class imbalance\n","What happens when some cases are overrepresented?\n"," - Fix: sample reweighing - sample more date on underrepresented\n"," - Fix: Los reweighing - mistakes are underrepresented\n"," - Fix: Batch selection - Choose randomly from classes so that ever batch has an equal number of points per class\n","\n","### What about Latent Features\n","How do we know which features to label?\n","The amount of labeling would be work intensive\n","\n","Use Variational autoencoders (see lecture 4)\n","1. learn latent structure.  i.e. angle of face image\n","2. estimate distribution.  We can undersample on dense areas of set and oversample on sparse areas\n","3. Adaptively guide learning as described in step 2\n","\n","Estimated joint distribution:\n","$$\\hat{Q}(z|X) \\approx\\Pi_i\\hat{Q}_i(z_i|X)$$\n","where:\n","  \n","$\\Pi_i$ is the independence to approximate\n","\n","and $\\hat{Q}_i(z_i|X)$ is the histogram for every latent variable $z_i$.  Thus we define\n","\n","The probability of selecting a datapoint\n","\n","$$W(z(x)|X) \\approx \\Pi_i\\frac{1}{\\hat{Q}_i(z_i|X) + \\alpha}$$\n","\n","where $\\alpha$ is a debiasing parameter.  As $\\alpha$ increases this probability goes to the uniform distro.  As it decrease we debias more strongly.\n","\n","The weight of the sample $W(z(x)|X)$ is used to adaptively resample.  See the missing slide saved image in to show the effect of alpha size.\n","\n","\n","## Uncertainty\n","### Type comparison\n","\n","| Feature\t| Aleatoric Uncertainty\t| Epistemic Uncertainty|\n","| --- | --- | --- |\n","| Nature of Uncertainty |\tInherent, irreducible variability in data\t|Model-related, reducible with more information|\n","|Causes |\tInherent stochastic factors (e.g., noise)\t|Lack of knowledge about the underlying model\n","|Quantifiability |\tQuantifiable, estimated directly from data |\tOften indirect estimation (e.g., Bayesian methods)\n","|Reducibility |\tIrreducible, remains present with ideal knowledge/model\t|Reducible with more information, better models|\n","|Handling in Models |\tIncorporated through probabilistic components, modeling data variability\t| Addressed through Bayesian methods, ensembling, dropout-based uncertainty |\n","|Practical Implications\t|Crucial for robust decision-making under inherent variability |\tImportant for model selection, improvement, and decision-making under uncertainty due to lack of knowledge |\n","\n","### Estimating Aleatoric uncertainty/Loss\n","- Learn a set of variances corresponding to the input.\n","- A.Uncertainty: $f_\\theta\\rightarrow\\hat{y},\\sigma^2$\n"," - $\\hat{y}$ : prediction\n"," - $\\sigma^2$ : variance.  Is not independant of input\n","\n","#### Loss function\n","Negative Log Likelihood (NLL) is a generalization of MSE to non-constant variance\n","\n","$$L=\\frac{1}{N} * \\sum_{i=1}^{N}\\frac{(\\hat{Y}_i-y_i)^2}{2\\sigma_i^2}+ln(\\sigma_i^2)$$\n","\n","- This allows us to understand how the $\\sigma$ and $y$ influence our uncertainty\n","\n","#### Example\n","In RGB images.  The highest alertoric uncertainty occurs at the edges/boundaries of objects withing the image.  this is irreducible\n","\n","### Epistemic Uncertainty\n","- Model dependant uncertainty.  Does the model have confidence in the prediction?\n","\n","- If you were to train an 'ensemble' of models with the same hyperparameters but with different weights, we would see different outputs.  A comparison of these outputs could be used to calculate uncertainty between outputs.\n"," - The problem with this is that it is cost intesive to create and compare multiple models.\n"," - Fix this with dropout layers\n"," - **KEEP** dropout layers at test time\n","\n","\n","## CAPSA - Themis AI project\n","- Model agnostic framework for risk estimation\n","- Nueral Network wrapper\n","- Added to training workflow.\n","- Calculates Biases, Uncertainty, and label noise\n","- Simplifies uncertainity calculation\n","\n","```python\n"," train, test = load_data()\n"," model = capsa.HistogramWrapper(model,...)\n"," model.train(train)\n"," preds,bias = model.predict(test)\n","```\n","\n","\n","\n"],"metadata":{"id":"noR4BFLb4jqh"}},{"cell_type":"markdown","source":["# Lecture 6: Deep Reinforcement Learning\n","- Data given in State:Action pairs\n","## Key Terms\n","- **Agent** - takes actions.  Algorithm is the agent\n","- **Environment** - the world in which the agent exists\n","- **Action** - $a_t$ A move our input the agent can make in the environement\n","- **Observation** - how the environment responds back with a change of state\n","- **State change** - $s_{t+1}$\n","- **Reward** - $r_t$ -feedback that measures the success or failure of the agents action\n"," - **Discounted total reward** : $R_t=\\sum_{i+t}^\\infty \\gamma^ir_i$\n"," - $\\gamma$ - dampening term makes future rewards worth less. $0<\\gamma<1$\n","- **Policy** $r(s)$ best action to take at state\n","\n","## Q-Function\n","$$Q(s_t,a_t) = ð”¼[R_t|S_t,a_t]$$\n","- Captures the expected total future reward an agent is state,$s$, can receive by executing a certain action, $a$.\n","- The Q-function gives us the action to take at a current state to maximize the reqard\n","- Strategy\n","$$\\pi(s)=argmax_a(Q(s,a))$$\n","- Learned with Deep learning\n","\n","##Value/Q- learning\n","Find $Q(s,a)$ where the state maximizes the Q-function\n","$a=argmax_aQ(s,a)$. i.e. optomize Qfunction\n","\n","**Target** $(r+\\gamma* max_{a'}Q(s',a'))$\n","Predicted Q(s,a)\n","\n","- **Q-loss** - Mean Squared Error loss bewteen target and output defined by:\n","$$L = ð”¼[||r+\\gamma*max_{a'}Q(s',a') - Q(s,a)||^2] $$\n","\n","### Downsides of Q-learning\n","- Complexity\n"," - can model scenarios where the actions space is discrete and small\n"," - Cannot handle continous action spaces\n","- Flexibility\n"," - Policy is deterministically computed from the Q function by maximizing the reward --> cannot learn stochastic policies\n","\n","##Policy Learning / Policy gradient algorithms\n","Find $\\pi(s)$ to optomize reward\n","Sample $a~\\pi(s)$\n","- Selecting the output which has the highest probaility of choosing the best policy.  B/c this is a distribution:\n","$$\\pi(s)~P(a|s)=\\sum_{a_i\\in A}P(a_i|s)=1$$\n","\n","### Loss function\n","$$loss = -logP(a_t|s_t)R_t$$\n","- if we get a lot of reward for an action that has a high probabilty.  We would continue to sample that action into the future\n","- If we sample an action with low reward we wont want to repeat that action\n","- The negative value of the **loss* function drives us to maximumns\n","-Apply to gradient descent\n","$$w'=w+\\nabla logP(a_t|s_t)R_t$$\n"],"metadata":{"id":"Gf-eXLie8B95"}}]}